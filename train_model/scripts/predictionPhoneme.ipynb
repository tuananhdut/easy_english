{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "import torch\n",
    "\n",
    "# ƒë·ªçc file √¢m thanh\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5afe7435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "# model_dir = \"vitouphy/wav2vec2-xls-r-300m-phoneme\"\n",
    "model_dir = \"../model/kaggle/working/phoneme_recognition\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_dir)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_dir)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33457cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aa': 1,\n",
       " 'ae': 2,\n",
       " 'ah': 3,\n",
       " 'aw': 4,\n",
       " 'ay': 5,\n",
       " 'b': 6,\n",
       " 'ch': 7,\n",
       " 'd': 8,\n",
       " 'dh': 9,\n",
       " 'dx': 10,\n",
       " 'eh': 11,\n",
       " 'er': 12,\n",
       " 'ey': 13,\n",
       " 'f': 14,\n",
       " 'g': 15,\n",
       " 'h#': 16,\n",
       " 'hh': 17,\n",
       " 'ih': 18,\n",
       " 'iy': 19,\n",
       " 'jh': 20,\n",
       " 'k': 21,\n",
       " 'l': 22,\n",
       " 'm': 23,\n",
       " 'n': 24,\n",
       " 'ng': 25,\n",
       " 'ow': 26,\n",
       " 'oy': 27,\n",
       " 'p': 28,\n",
       " 'r': 29,\n",
       " 's': 30,\n",
       " 'sh': 31,\n",
       " 't': 32,\n",
       " 'th': 33,\n",
       " 'uh': 34,\n",
       " 'uw': 35,\n",
       " 'v': 36,\n",
       " 'w': 37,\n",
       " 'y': 38,\n",
       " 'z': 39,\n",
       " '|': 0,\n",
       " '[UNK]': 40,\n",
       " '[PAD]': 41,\n",
       " '<s>': 42,\n",
       " '</s>': 43,\n",
       " '<unk>': 44,\n",
       " '<pad>': 45}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l·∫•y b·∫£ng m√£ √¢m v·ªã t·ª´ file csv\n",
    "vocab = processor.tokenizer.get_vocab()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94070e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1185, -0.1185, -0.1185,  ..., -0.1185, -0.1185, -0.1185]])\n",
      "                0\n",
      "0    input_values\n",
      "1  attention_mask\n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc file √¢m thanh (16kHz, mono)\n",
    "audio_path = \"../temp/output.wav\"\n",
    "speech, sample_rate = sf.read(audio_path)\n",
    "\n",
    "if sample_rate != 16000:\n",
    "    speech = librosa.resample(speech, orig_sr=sample_rate, target_sr=16000)\n",
    "    sample_rate = 16000\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi √¢m thanh th√†nh input features\n",
    "inputs = processor(speech, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "print(inputs['input_values'])\n",
    "print(pd.DataFrame(inputs))\n",
    "input_values = inputs.input_values.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cdf848c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_text: dawnlowdxihng\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# D·ª± ƒëo√°n ID token\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode ra text v√† t√°ch th√†nh m·∫£ng token\n",
    "predicted_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "predicted_text = predicted_text.replace(\"h#\", \"\")\n",
    "predicted_text = predicted_text.replace(\"[PAD]\", \"\")\n",
    "predicted_text = predicted_text.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "# In ra k·∫øt qu·∫£\n",
    "print(\"predicted_text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "514a328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dawnlowdihng\n",
      "Predicted Text: dawnlowdxihng\n"
     ]
    }
   ],
   "source": [
    "import pronouncing\n",
    "import re\n",
    "\n",
    "def sentence_to_phonemes(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    phoneme_sequence = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        phones = pronouncing.phones_for_word(word)\n",
    "        if phones:\n",
    "            # L·∫•y c√°ch ph√°t √¢m ƒë·∫ßu ti√™n v√† t√°ch √¢m v·ªã\n",
    "            phonemes = phones[0].split()\n",
    "            # Lo·∫°i b·ªè s·ªë tr·ªçng √¢m (0, 1, 2)\n",
    "            cleaned_phonemes = [re.sub(r'\\d', '', p) for p in phonemes]\n",
    "            phoneme_sequence  = phoneme_sequence.join(cleaned_phonemes)\n",
    "        else:\n",
    "            # N·∫øu kh√¥ng t√¨m th·∫•y √¢m v·ªã\n",
    "            phoneme_sequence.append(f\"[UNK:{word}]\")\n",
    "\n",
    "    return phoneme_sequence.lower()\n",
    "\n",
    "# üîé V√≠ d·ª• s·ª≠ d·ª•ng\n",
    "sentence = \"Downloading\"\n",
    "phonemes = sentence_to_phonemes(sentence)\n",
    "print(phonemes)\n",
    "print(\"Predicted Text:\", predicted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "26c68778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phonemes_verbose(target, predicted):\n",
    "    # chuy·ªÉn ƒë·ªïi c√°c √¢m v·ªã th√†nh ch·ªØ th∆∞·ªùng\n",
    "    target = [p.lower() for p in target]\n",
    "\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "\n",
    "    # Danh s√°ch c√°c c·∫∑p t∆∞∆°ng ƒë∆∞∆°ng\n",
    "    equivalent_pairs = {\n",
    "        (\"ih\", \"ah\"), (\"ah\", \"ih\"),\n",
    "        (\"ih\", \"eh\"), (\"eh\", \"ih\"),\n",
    "        (\"ih\", \"iy\"), (\"iy\", \"ih\")\n",
    "    }\n",
    "\n",
    "    min_len = min(len(target), len(predicted))\n",
    "    target = target[:min_len]\n",
    "    predicted = predicted[:min_len]\n",
    "\n",
    "    correct = 0\n",
    "    total = min_len\n",
    "    mistakes = []\n",
    "\n",
    "    for i, (t, p) in enumerate(zip(target, predicted)):\n",
    "        if t == p or (t, p) in equivalent_pairs:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((i, t, p))\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"\\nüîç T·ªïng s·ªë: {total}, ƒê√∫ng: {correct}, Sai: {len(mistakes)}\")\n",
    "    print(f\"üéØ Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    if mistakes:\n",
    "        print(\"\\n‚ùå V·ªã tr√≠ sai:\")\n",
    "        for i, t, p in mistakes:\n",
    "            print(f\"  V·ªã tr√≠ {i}: expected '{t}', predicted '{p}'\")\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033c773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "from jiwer import cer\n",
    "cer_value = cer(phonemes, predicted_text)\n",
    "print(cer_value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c438eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: ['d', 'a', 'w', 'n', 'l', 'o', 'w', 'd', 'i', 'h', 'n', 'g']\n",
      "Predicted: ['dawn', 'lowdxihng']\n",
      "\n",
      "üîç T·ªïng s·ªë: 2, ƒê√∫ng: 0, Sai: 2\n",
      "üéØ Accuracy: 0.00%\n",
      "\n",
      "‚ùå V·ªã tr√≠ sai:\n",
      "  V·ªã tr√≠ 0: expected 'd', predicted 'dawn'\n",
      "  V·ªã tr√≠ 1: expected 'a', predicted 'lowdxihng'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
