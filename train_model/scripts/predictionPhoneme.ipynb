{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b6f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\DATN\\SourceCode\\train_model\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor\n",
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5afe7435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "# Đường dẫn đến thư mục chứa mô hình và processor\n",
    "model_dir = \"../model/kaggle/working/phoneme_recognition\"\n",
    "\n",
    "# Load processor từ thư mục\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_dir)\n",
    "\n",
    "# Load mô hình từ thư mục\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_dir)\n",
    "\n",
    "# Chuyển mô hình sang chế độ đánh giá\n",
    "model.eval()\n",
    "\n",
    "# Chuyển sang GPU nếu có\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2c815",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchaudio'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Load và xử lý âm thanh\u001b[39;00m\n\u001b[32m      3\u001b[39m audio_path = \u001b[33m\"\u001b[39m\u001b[33mpath/to/your_audio.wav\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchaudio'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torchaudio\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# Định nghĩa các hàm của bạn\n",
    "PAD_ID = None  # Sẽ cập nhật sau khi load processor\n",
    "EMPTY_ID = None\n",
    "\n",
    "def collapse_tokens(tokens: List[Union[str, int]]) -> List[Union[str, int]]:\n",
    "    prev_token = None\n",
    "    out = []\n",
    "    for token in tokens:\n",
    "        if token != prev_token and prev_token is not None:\n",
    "            out.append(prev_token)\n",
    "        prev_token = token\n",
    "    return out\n",
    "\n",
    "def clean_token_ids(token_ids: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Remove [PAD] and collapse duplicated token_ids\n",
    "    \"\"\"\n",
    "    token_ids = [x for x in token_ids if x not in [PAD_ID, EMPTY_ID]]\n",
    "    token_ids = collapse_tokens(token_ids)\n",
    "    return token_ids\n",
    "\n",
    "def decode_operations(predicted_chr: str, label_chr: str, editops: List[Tuple[str, int, int]]) -> List[Tuple[str, str, str]]:\n",
    "    ops = []\n",
    "    for editop in editops:\n",
    "        op, pred_idx, label_idx = editop\n",
    "        \n",
    "        if op == \"insert\":\n",
    "            label_token = tokenizer.decode(ord(label_chr[label_idx]), group_tokens=False)\n",
    "            ops.append((op, label_token, label_token))\n",
    "        elif op == \"delete\":\n",
    "            pred_token = tokenizer.decode(ord(predicted_chr[pred_idx]), group_tokens=False)\n",
    "            ops.append((op, pred_token, pred_token))\n",
    "        else:\n",
    "            label_token = tokenizer.decode(ord(label_chr[label_idx]), group_tokens=False)\n",
    "            pred_token = tokenizer.decode(ord(predicted_chr[pred_idx]), group_tokens=False)\n",
    "            ops.append((op, pred_token, label_token))\n",
    "            \n",
    "    return ops\n",
    "\n",
    "# Load mô hình và processor\n",
    "model_dir = \"../model/kaggle/working/phoneme_recognition\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_dir)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_dir)\n",
    "tokenizer = processor.tokenizer  # Lấy tokenizer từ processor\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Cập nhật PAD_ID và EMPTY_ID\n",
    "PAD_ID = tokenizer.encode(\"[PAD]\")[0]\n",
    "EMPTY_ID = tokenizer.encode(\" \")[0]\n",
    "\n",
    "# Load file âm thanh\n",
    "audio_path = \"../temp/test1.wav\"  # Thay bằng đường dẫn file âm thanh\n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "if sample_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "    waveform = resampler(waveform)\n",
    "\n",
    "# Chuẩn bị đầu vào\n",
    "inputs = processor(waveform.squeeze(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Dự đoán\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs[\"input_values\"]).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)[0].cpu().tolist()  # Chuyển sang list\n",
    "\n",
    "# Làm sạch token IDs\n",
    "cleaned_ids = clean_token_ids(predicted_ids)\n",
    "\n",
    "# Giải mã sang chuỗi ký tự\n",
    "predicted_chr = processor.decode(cleaned_ids)\n",
    "\n",
    "print(\"Predicted transcription:\", predicted_chr)\n",
    "\n",
    "# # Nhãn thật (ground truth) - thay bằng nhãn thực tế của bạn\n",
    "# label_chr = \"the cat\"  # Ví dụ, thay bằng nhãn thật của file âm thanh\n",
    "\n",
    "# # Tính toán edit operations bằng thư viện Levenshtein (cần cài đặt: pip install Levenshtein)\n",
    "# from Levenshtein import editops\n",
    "# edit_operations = editops(predicted_chr, label_chr)\n",
    "\n",
    "# # Phân tích các thao tác chỉnh sửa\n",
    "# operations = decode_operations(predicted_chr, label_chr, edit_operations)\n",
    "\n",
    "# # In kết quả\n",
    "# print(\"Predicted transcription:\", predicted_chr)\n",
    "# print(\"Ground truth:\", label_chr)\n",
    "# print(\"Edit operations:\", operations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
